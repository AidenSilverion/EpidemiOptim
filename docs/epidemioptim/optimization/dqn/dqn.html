<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>epidemioptim.optimization.dqn.dqn API documentation</title>
<meta name="description" content="Adapted from https://github.com/higgsfield/RL-Adventure" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>epidemioptim.optimization.dqn.dqn</code></h1>
</header>
<section id="section-intro">
<p>Adapted from <a href="https://github.com/higgsfield/RL-Adventure">https://github.com/higgsfield/RL-Adventure</a></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Adapted from https://github.com/higgsfield/RL-Adventure
&#34;&#34;&#34;


import os

import pickle
import numpy as np
import torch
import torch.optim as optim
import torch.autograd as ag

from epidemioptim.optimization.shared.replay_buffer import ReplayBuffer
from epidemioptim.optimization.shared.rollout import run_rollout
from epidemioptim.optimization.base_algorithm import BaseAlgorithm
from epidemioptim.optimization.shared.networks import Critic
from epidemioptim.utils import Logger, compute_pareto_front

try:
    import sobol_seq
    def sample_goals(n, dim_goal):
        return sobol_seq.i4_sobol_generate(dim_goal, n)
except:
    def sample_goals(n, dim_goal):
        return np.random.uniform(0, 1, size=(n, dim_goal))


class DQN(BaseAlgorithm):
    def __init__(self, env, params):
        &#34;&#34;&#34;
        DQN algorithm from Mnih et al., 2015.
        This implementation includes mechanisms from Universal Value Function Approximators (Schaul et al., 2015)
        and Agent57 (Badia et al., 2019).

        Parameters
        ----------
        env: BaseEnv
            Learning environment.
        params: dict
            Dictionary of parameters.

        Attributes
        ----------
        batch_size: int
            Batch size.
        gamma: float
            Discount factor in [0, 1].
        layers: tuple of ints
            Describes sizes of hidden layers of the critics.
        goal_conditioned: bool
            Whether the algorithm is goal conditioned or not. This is the idea behind UVFA (Schaul et al., 2015).
        replace_target_cnt: int
            Frequency with which target critics should be replaced by current critics (in learning steps).
        logdir: str
            Logging directory
        save_policy_every: int
            Frequency to save policy (in episodes).
        eval_and_log_every: int
            Frequency to pring logs (in episodes).
        n_evals_if_stochastic: int
            Number of evaluation episodes if the environment is stochastic.
        stochastic: bool
            Whether the environment is stochastic.
        epsilon: float
            Probability to sample a random action (epsilon-greedy exploration). This is set to 0 during evaluation.
        dims: dict
            Dimensions of states and actions.
        cost_function: BaseMultiCostFunction
            Multi-cost function.
        nb_costs: int
            Number of cost functions
        use_constraints: bool
            Whether the algorithm uses constraints.
        pareto_size: int
            Number of random goals to be sampled to generate the pareto front.
        &#34;&#34;&#34;
        super(DQN, self).__init__(env, params)

        # Save parameters
        self.batch_size = self.algo_params[&#39;batch_size&#39;]
        self.gamma = self.algo_params[&#39;gamma&#39;]
        self.layers = tuple(self.algo_params[&#39;layers&#39;])
        self.goal_conditioned = self.algo_params[&#39;goal_conditioned&#39;]
        self.replace_target_cnt = self.algo_params[&#39;replace_target_count&#39;]
        self.logdir = params[&#39;logdir&#39;]
        self.save_policy_every = self.algo_params[&#39;save_policy_every&#39;]
        self.eval_and_log_every = self.algo_params[&#39;eval_and_log_every&#39;]
        self.n_evals_if_stochastic = self.algo_params[&#39;n_evals_if_stochastic&#39;]
        self.stochastic = params[&#39;model_params&#39;][&#39;stochastic&#39;]
        self.epsilon = self.algo_params[&#39;epsilon_greedy&#39;]
        self.cost_function = self.env.unwrapped.cost_function
        self.nb_costs = self.env.unwrapped.cost_function.nb_costs
        self.use_constraints = self.cost_function.use_constraints
        self.pareto_size = self.algo_params[&#39;pareto_size&#39;]
        self.is_multi_obj = True if self.goal_conditioned else False  # DQN is not a multi-obj algorithm, unless it is goal-conditioned
        self.dims = dict(s=env.observation_space.shape[0],
                         a=env.action_space.n)

        #

        if self.goal_conditioned:
            self.goal_dim = self.env.unwrapped.cost_function.goal_dim
            eval_goals = self.cost_function.get_eval_goals(1)
            goals, index, inverse = np.unique(eval_goals, return_inverse=True, return_index=True, axis=0)
            goal_keys = [str(g) for g in goals]
        else:
            self.goal_dim = 0
            goal_keys = [str(self.cost_function.beta)]

        # Initialize Logger.
        if self.logdir:
            os.makedirs(self.logdir + &#39;models/&#39;, exist_ok=True)
            stats_keys = [&#39;mean_agg&#39;, &#39;std_agg&#39;] + [&#39;mean_C{}&#39;.format(i) for i in range(self.nb_costs)] + [&#39;std_C{}&#39;.format(i) for i in range(self.nb_costs)]

            keys = [&#39;Episode&#39;, &#39;Best score so far&#39;, &#39;Eval score&#39;]
            for k in goal_keys:
                for s in stats_keys:
                    keys.append(&#39;Eval, g: &#39; + k + &#39;: &#39; + s)
            keys += [&#39;Loss {}&#39;.format(i + 1) for i in range(self.nb_costs)] + [&#39;Train, Cost {}&#39;.format(i + 1) for i in range(self.nb_costs)] + [&#39;Train, Aggregated cost&#39;]
            self.logger = Logger(keys=keys,
                                 logdir=self.logdir)

        # Initialize replay buffer
        self.replay_buffer = ReplayBuffer(self.algo_params[&#39;buffer_size&#39;])

        # Initialize critics
        self.Q_eval = Critic(n_critics=self.nb_costs,
                             dim_state=self.dims[&#39;s&#39;],
                             dim_goal=self.goal_dim,
                             dim_actions=self.dims[&#39;a&#39;],
                             goal_ids=((), ()),  # no goal, the mixing parameters comes after the critic
                             layers=self.layers)
        self.Q_next = Critic(n_critics=self.nb_costs,
                             dim_state=self.dims[&#39;s&#39;],
                             dim_goal=self.goal_dim,
                             dim_actions=self.dims[&#39;a&#39;],
                             goal_ids=((), ()),
                             layers=self.layers)

        # Initialize optimizers.
        self.optimizers = [optim.Adam(q.parameters(), lr=self.algo_params[&#39;lr&#39;]) for q in self.Q_eval.qs]

        # If we use constraint, we train a Q-network per constraint using a negative reward of -1 whenever the constraint is violated.
        # This network learns to estimate the number of times the constraint will be violated in the future.
        # We then use it to guide action selection, selecting the action that does not violate the constraint when the other does,
        # or the action that minimizes constraint violation when both action lead to constraint violations.
        if self.use_constraints:
            self.nb_constraints = len(self.cost_function.constraints_ids)
            self.Q_eval_constraints = Critic(n_critics=self.nb_constraints,
                                             dim_state=self.dims[&#39;s&#39;],
                                             dim_goal=self.goal_dim,
                                             dim_actions=self.dims[&#39;a&#39;],
                                             goal_ids=self.cost_function.constraints_ids,
                                             layers=self.layers)
            self.Q_next_constraints = Critic(n_critics=self.nb_constraints,
                                             dim_state=self.dims[&#39;s&#39;],
                                             dim_goal=self.goal_dim,
                                             dim_actions=self.dims[&#39;a&#39;],
                                             goal_ids=self.cost_function.constraints_ids,
                                             layers=self.layers)
            self.optimizers_constraints = [optim.Adam(q.parameters(), lr=self.algo_params[&#39;lr&#39;]) for q in self.Q_eval_constraints.qs]
        else:
            self.nb_constraints = 0

        # Initialize counters
        self.learn_step_counter = 0
        self.env_step_counter = 0
        self.episode = 0
        self.best_cost = np.inf
        self.aggregated_costs = []
        self.costs = []

    def _replace_target_network(self):
        &#34;&#34;&#34;
        Replaces the target network with the evaluation network every &#39;self.replace_target_cnt&#39; learning steps.
        &#34;&#34;&#34;
        if self.replace_target_cnt is not None and self.learn_step_counter % self.replace_target_cnt == 0:
            self.Q_next.set_goal_params(self.Q_eval.get_params())
        if self.use_constraints:
            if self.replace_target_cnt is not None and self.learn_step_counter % self.replace_target_cnt == 0:
                self.Q_next_constraints.set_goal_params(self.Q_eval_constraints.get_params())

    def _update(self, batch_size):
        &#34;&#34;&#34;
        Performs network updates according to the DQN algorithm.
        Here we update several critics: one for each cost and one for each constraint.
        We then use these critics at decision time: first filtering actions that are not expected to violate constraints,
        then selecting actions that maximize a convex combination of the costs as expressed by the mixing parameter beta.
        Beta can be provided by the experimenter (dqn) or selected by the agent (goal_dqn).

        Parameters
        ----------
        batch_size: int
            Batch size

        Returns
        -------
        loss

        &#34;&#34;&#34;

        # Reset gradients of optimizes
        for opt in self.optimizers:
            opt.zero_grad()

        if self.use_constraints:
            for opt in self.optimizers_constraints:
                opt.zero_grad()

        # Update target network.
        self._replace_target_network()

        # Sample a batch
        state, action, cost_aggregated, costs, next_state, goal, done, constraints = self.replay_buffer.sample(batch_size)

        # Concatenate goal if the policy is goal conditioned (might not be used afterwards).
        if self.goal_conditioned:
            state = ag.Variable(torch.FloatTensor(np.float32(np.concatenate([state, goal], axis=1))))
            next_state = ag.Variable(torch.FloatTensor(np.float32(np.concatenate([next_state, goal], axis=1))))
        else:
            state = ag.Variable(torch.FloatTensor(np.float32(state)))
            next_state = ag.Variable(torch.FloatTensor(np.float32(next_state)))

        action = ag.Variable(torch.LongTensor(action))
        indices = np.arange(self.batch_size)

        rewards = [- ag.Variable(torch.FloatTensor(c_func.scale(c))) for c_func, c in zip(self.cost_function.costs, costs.transpose())]


        q_preds = self.Q_eval.forward(state)
        q_preds = [q_p[indices, action] for q_p in q_preds]
        q_nexts = self.Q_next.forward(next_state)
        q_evals = self.Q_eval.forward(next_state)

        max_actions = [torch.argmax(q_ev, dim=1) for q_ev in q_evals]

        q_targets = [r + self.gamma * q_nex[indices, max_act] for r, q_nex, max_act in zip(rewards, q_nexts, max_actions)]
        losses = [(q_pre - ag.Variable(q_targ.data)).pow(2).mean() for q_pre, q_targ in zip(q_preds, q_targets)]
        for loss in losses:
            loss.backward()

        for opt in self.optimizers:
            opt.step()

        if self.use_constraints:
            constraints = [ag.Variable(torch.FloatTensor(constraints[:, i])) for i in range(self.nb_constraints)]

            q_preds = list(self.Q_eval_constraints.forward(state))
            q_preds = [q_p[indices, action.squeeze()] for q_p in q_preds]
            q_nexts = self.Q_next_constraints.forward(next_state)
            q_evals = self.Q_eval_constraints.forward(next_state)

            for i_q in range(self.nb_constraints):
                max_actions = torch.argmax(q_evals[i_q], dim=1)
                q_target = - constraints[i_q] + 1 * q_nexts[i_q][indices, max_actions]
                losses.append((q_preds[i_q] - ag.Variable(q_target.data)).pow(2).mean())
                losses[-1].backward()

            for opt in self.optimizers_constraints:
                opt.step()
        self.learn_step_counter += 1
        return losses

    def store_episodes(self, episodes):
        lengths = []
        for e in episodes:
            for t in range(e[&#39;env_states&#39;].shape[0] - 1):
                self.replay_buffer.push(state=e[&#39;env_states&#39;][t],
                                        action=e[&#39;actions&#39;][t],
                                        aggregated_cost=e[&#39;aggregated_costs&#39;][t],
                                        costs=e[&#39;costs&#39;][t],
                                        next_state=e[&#39;env_states&#39;][t + 1],
                                        constraints=e[&#39;constraints&#39;][t],
                                        goal=e[&#39;goal&#39;],
                                        done=e[&#39;dones&#39;][t])
            lengths.append(e[&#39;env_states&#39;].shape[0] - 1)
        return lengths

    def act(self, state, deterministic=False):
        &#34;&#34;&#34;
        Policy that uses the learned critics.

        Parameters
        ----------
        state: 1D nd.array
            Current state.
        deterministic: bool
            Whether the policy should be deterministic (e.g. in evaluation mode).

        Returns
        -------
        action: nd.array
            Action vector.
        q_constraints: nd.array
            Values of the critics estimating the expected constraint violations.
        &#34;&#34;&#34;
        if np.random.rand() &gt; self.epsilon or deterministic:
            if self.use_constraints:
                # If we use constraint, then the set of action is filtered by the constraint critics
                # so that we pick an action that is not expected to lead to constraint violation.
                # In the remaining actions, we take the one that maximizes the mixture of critics
                # that evaluate the values of each negative costs expected in the future.
                # If all actions lead to constraint violation, we chose the one that minimizes it.
                with torch.no_grad():
                    state = ag.Variable(torch.FloatTensor(state).unsqueeze(0))
                    q_value1, q_value2 = self.Q_eval.forward(state)
                    beta = self.cost_function.beta
                    q_constraints = torch.cat(self.Q_eval_constraints.forward(state)).numpy()
                    q_constraints_clipped = q_constraints.clip(max=0) # clamp to 0 (q value must be neg)
                    q_constraints_worst = q_constraints_clipped.min(axis=0)
                    valid_ids = np.argwhere(q_constraints_worst &gt; -1).flatten()
                    if valid_ids.size == 0:
                        action = np.argmax(q_constraints.sum(axis=0))
                    else:
                        q_value = (1 - beta) * q_value1[0, valid_ids] + beta * q_value2[0, valid_ids]
                        action = valid_ids[np.argmax(q_value.numpy())]
            else:
                # If no constraint, then the best action is the one that maximizes
                # the mixture of values with the chose mixing parameter beta (either by experimenter or by agent).
                with torch.no_grad():
                    state = ag.Variable(torch.FloatTensor(state).unsqueeze(0))
                    q_value1, q_value2 = self.Q_eval.forward(state)
                    beta = self.cost_function.beta
                    q_value = (1 - beta) * q_value1 + beta * q_value2
                action = int(q_value.max(1)[1].data[0])
                q_constraints = None
        else:
            # Epsilon-greedy exploration, random action with probability epsilon.
            action = np.random.randint(self.dims[&#39;a&#39;])
            q_constraints = None
        return np.atleast_1d(action), q_constraints

    def update(self):
        &#34;&#34;&#34;
        Update the algorithm.

        &#34;&#34;&#34;
        if self.env_step_counter &gt; 0:
            losses = self._update(self.batch_size)
            return [np.atleast_1d(l.data)[0] for l in losses]
        else:
            return [0] * (2 + self.nb_constraints)

    def save_model(self, path):
        &#34;&#34;&#34;
        Extract model state dicts and save them.
        Parameters
        ----------
        path: str
            Saving path.

        &#34;&#34;&#34;
        q_eval = self.Q_eval.get_model()
        to_save = [q_eval]
        if self.use_constraints:
            q_constraints = self.Q_eval_constraints.get_model()
            to_save.append(q_constraints)
        with open(path, &#39;wb&#39;) as f:
            torch.save(to_save, f)

    def load_model(self, path):
        &#34;&#34;&#34;
        Load model from file and feed critics&#39; state dicts.
        Parameters
        ----------
        path: str
            Loading path
        &#34;&#34;&#34;
        with open(path, &#39;rb&#39;) as f:
            out = torch.load(f)
        try:
            self.Q_eval.set_model(out[0])
        except:
            self.Q_eval.set_model(out)

        if self.use_constraints:
            self.Q_eval_constraints.set_model(out[1])

    def learn(self, num_train_steps):
        &#34;&#34;&#34;
        Main training loop.

        Parameters
        ----------
        num_train_steps: int
            Number of training steps (environment steps)

        Returns
        -------

        &#34;&#34;&#34;

        while self.env_step_counter &lt; num_train_steps:
            if self.goal_conditioned:
                goal = self.env.unwrapped.sample_cost_function_params()
            else:
                goal = None

            episodes = run_rollout(policy=self,
                                   env=self.env,
                                   n=1,
                                   goal=goal,
                                   eval=False,
                                   additional_keys=(&#39;costs&#39;, &#39;constraints&#39;),
                                   )
            lengths = self.store_episodes(episodes)
            self.env_step_counter += np.sum(lengths)
            self.episode += 1

            self.aggregated_costs.append(np.sum(episodes[0][&#39;aggregated_costs&#39;]))
            self.costs.append(np.sum(episodes[0][&#39;costs&#39;], axis=0))

            # Update
            if len(self.replay_buffer) &gt; self.batch_size:
                update_losses = []
                for _ in range(int(np.sum(lengths) * 0.5)):
                    update_losses.append(self.update())
                update_losses = np.array(update_losses)
                losses = update_losses.mean(axis=0)
            else:
                losses = [np.nan] * 2

            if self.episode % self.eval_and_log_every == 0:
                # Run evaluations
                new_logs, eval_costs = self.evaluate(n=self.n_evals_if_stochastic if self.stochastic else 1)
                # Compute train scores
                train_agg_cost = np.mean(self.aggregated_costs)
                train_costs = np.array(self.costs).mean(axis=0)
                self.log(self.episode, new_logs, losses, train_agg_cost, train_costs)
                # Reset training score tracking
                self.aggregated_costs = []
                self.costs = []

            if self.episode % self.save_policy_every == 0:
                self.save_model(self.logdir + &#39;/models/policy_{}.cp&#39;.format(self.episode))
        self.evaluate_pareto()
        print(&#39;Run has terminated successfully&#39;)

    def evaluate(self, n=None, goal=None, best=None, reset_same_model=False):
        # run eval
        if n is None:
            n = self.n_evals_if_stochastic if self.env.unwrapped.stochastic else 1
        if self.goal_conditioned:
            if goal is not None:
                eval_goals = np.array([goal] * n)
            else:
                eval_goals = self.cost_function.get_eval_goals(n)

            n = eval_goals.shape[0]
        else:
            eval_goals = None
        eval_episodes = run_rollout(policy=self,
                                    env=self.env,
                                    n=n,
                                    goal=eval_goals,
                                    eval=True,
                                    reset_same_model=reset_same_model,
                                    additional_keys=(&#39;costs&#39;, &#39;constraints&#39;),
                                    )
        new_logs, costs = self.compute_eval_score(eval_episodes, eval_goals)
        return new_logs, costs

    def compute_eval_score(self, eval_episodes, eval_goals):
        aggregated_costs = [np.sum(e[&#39;aggregated_costs&#39;]) for e in eval_episodes]
        costs = np.array([np.sum(e[&#39;costs&#39;], axis=0) for e in eval_episodes])

        new_logs = dict()
        if self.goal_conditioned:
            goals, index, inverse = np.unique(eval_goals, return_inverse=True, return_index=True, axis=0)
            agg_means = []
            for g, i in zip(goals, np.arange(index.size)):
                ind_g = np.argwhere(inverse == i).flatten()
                costs_mean = np.mean(costs[ind_g], axis=0)
                costs_std = np.std(costs[ind_g], axis=0)
                agg_rew_mean = np.mean(np.array(aggregated_costs)[ind_g], axis=0)
                agg_rew_std = np.std(np.array(aggregated_costs)[ind_g], axis=0)
                for i_r in range(self.nb_costs):
                    new_logs[&#39;Eval, g: &#39; + str(g) + &#39;: &#39; + &#39;mean_C{}&#39;.format(i_r)] = costs_mean[i_r]
                    new_logs[&#39;Eval, g: &#39; + str(g) + &#39;: &#39; + &#39;std_C{}&#39;.format(i_r)] = costs_std[i_r]
                new_logs[&#39;Eval, g: &#39; + str(g) + &#39;: &#39; + &#39;mean_agg&#39;] = agg_rew_mean
                new_logs[&#39;Eval, g: &#39; + str(g) + &#39;: &#39; + &#39;std_agg&#39;] = agg_rew_std
                agg_means.append(agg_rew_mean)
            new_logs[&#39;Eval score&#39;] = np.mean(agg_means)
        else:
            costs_mean = np.mean(np.atleast_2d(costs), axis=0)
            costs_std = np.std(np.atleast_2d(costs), axis=0)
            for i_r in range(self.nb_costs):
                new_logs[&#39;Eval, g: &#39; + str(self.cost_function.beta) + &#39;: &#39; + &#39;mean_C{}&#39;.format(i_r)] = costs_mean[i_r]
                new_logs[&#39;Eval, g: &#39; + str(self.cost_function.beta) + &#39;: &#39; + &#39;std_C{}&#39;.format(i_r)] = costs_std[i_r]
            new_logs[&#39;Eval score&#39;] = np.mean(aggregated_costs)
            new_logs[&#39;Eval, g: &#39; + str(self.cost_function.beta) + &#39;: &#39; + &#39;mean_agg&#39;] = np.mean(aggregated_costs)
            new_logs[&#39;Eval, g: &#39; + str(self.cost_function.beta) + &#39;: &#39; + &#39;std_agg&#39;] = np.mean(aggregated_costs)

        return new_logs, costs

    def log(self, episode, new_logs, losses, train_agg_cost, train_costs):
        if new_logs[&#39;Eval score&#39;] &lt; self.best_cost:
            self.best_cost = new_logs[&#39;Eval score&#39;]
            self.save_model(self.logdir + &#39;/models/best_model.cp&#39;)

        train_log_dict = {&#39;Episode&#39;: episode, &#39;Best score so far&#39;: self.best_cost}
        for i in range(self.nb_costs):
            train_log_dict[&#39;Loss {}&#39;.format(i + 1)] = losses[i]
            train_log_dict[&#39;Train, Cost {}&#39;.format(i + 1)] = train_costs[i]
            train_log_dict[&#39;Train, Aggregated cost&#39;] = train_agg_cost

        new_logs.update(train_log_dict)
        self.logger.add(new_logs)
        self.logger.print_last()
        self.logger.save()

    def evaluate_pareto(self, load_model=True):
        if load_model:
            self.load_model(self.logdir + &#39;/models/best_model.cp&#39;)
        if self.goal_conditioned:
            print(&#39;----------------\nForming pareto front&#39;)
            goals = sample_goals(self.pareto_size, self.cost_function.goal_dim)

            res = dict()

            costs_mean = []
            costs_std = []
            n = self.n_evals_if_stochastic if self.env.unwrapped.stochastic else 1
            for i_g, g in enumerate(goals):
                if (i_g + 1) % 20 == 0:
                    print(&#39;\t{:.2f} %&#39;.format((i_g + 1)/goals.shape[0] * 100))
                gs = np.atleast_2d(np.array([g for _ in range(n)]))
                if gs.shape[0] != n:
                    gs = gs.transpose()
                episodes = run_rollout(policy=self,
                                       env=self.env,
                                       n=n,
                                       goal=gs,
                                       eval=True,
                                       additional_keys=[&#39;costs&#39;],
                                       )

                costs = np.array([np.array(e[&#39;costs&#39;]).sum(axis=0) for e in episodes])
                costs_mean.append(costs.mean(axis=0))
                costs_std.append(costs.std(axis=0))
            res[&#39;F_all&#39;] = np.array(costs_mean)
            res[&#39;F_std_all&#39;] = np.array(costs_std)
            res[&#39;G_all&#39;] = goals

            front_ids = compute_pareto_front(costs_mean)
            costs_mean = np.array(costs_mean)
            costs_std = np.array(costs_std)
            costs_std = costs_std[front_ids]
            costs_mean = costs_mean[front_ids]
            res[&#39;F&#39;] = costs_mean
            res[&#39;F_std&#39;] = costs_std

            with open(self.logdir + &#39;res_eval.pk&#39;, &#39;wb&#39;) as f:
                pickle.dump(res, f)
        else:
            print(&#39;----------------\nForming pareto front&#39;)

            res = dict()
            costs_mean = []
            costs_std = []
            n = self.n_evals_if_stochastic if self.env.unwrapped.stochastic else 1
            episodes = run_rollout(policy=self,
                                   env=self.env,
                                   n=n,
                                   eval=True,
                                   additional_keys=[&#39;costs&#39;],
                                   )

            costs = np.array([np.array(e[&#39;costs&#39;]).sum(axis=0) for e in episodes])
            costs_mean.append(costs.mean(axis=0))
            costs_std.append(costs.std(axis=0))
            res[&#39;F&#39;] = np.array(costs_mean)
            res[&#39;F_std&#39;] = np.array(costs_std)
            for k in list(res.keys()):
                res[k + &#39;_all&#39;] = res[k]
            res[&#39;G_all&#39;] = np.array([[self.cost_function.beta_default for _ in range(len(costs_mean))]])

            with open(self.logdir + &#39;res_eval.pk&#39;, &#39;wb&#39;) as f:
                pickle.dump(res, f)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="epidemioptim.optimization.dqn.dqn.sample_goals"><code class="name flex">
<span>def <span class="ident">sample_goals</span></span>(<span>n, dim_goal)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_goals(n, dim_goal):
    return sobol_seq.i4_sobol_generate(dim_goal, n)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="epidemioptim.optimization.dqn.dqn.DQN"><code class="flex name class">
<span>class <span class="ident">DQN</span></span>
<span>(</span><span>env, params)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>DQN algorithm from Mnih et al., 2015.
This implementation includes mechanisms from Universal Value Function Approximators (Schaul et al., 2015)
and Agent57 (Badia et al., 2019).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>env</code></strong> :&ensp;<code>BaseEnv</code></dt>
<dd>Learning environment.</dd>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary of parameters.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Batch size.</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>Discount factor in [0, 1].</dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>tuple</code> of <code>ints</code></dt>
<dd>Describes sizes of hidden layers of the critics.</dd>
<dt><strong><code>goal_conditioned</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the algorithm is goal conditioned or not. This is the idea behind UVFA (Schaul et al., 2015).</dd>
<dt><strong><code>replace_target_cnt</code></strong> :&ensp;<code>int</code></dt>
<dd>Frequency with which target critics should be replaced by current critics (in learning steps).</dd>
<dt><strong><code>logdir</code></strong> :&ensp;<code>str</code></dt>
<dd>Logging directory</dd>
<dt><strong><code>save_policy_every</code></strong> :&ensp;<code>int</code></dt>
<dd>Frequency to save policy (in episodes).</dd>
<dt><strong><code>eval_and_log_every</code></strong> :&ensp;<code>int</code></dt>
<dd>Frequency to pring logs (in episodes).</dd>
<dt><strong><code>n_evals_if_stochastic</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of evaluation episodes if the environment is stochastic.</dd>
<dt><strong><code>stochastic</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the environment is stochastic.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>Probability to sample a random action (epsilon-greedy exploration). This is set to 0 during evaluation.</dd>
<dt><strong><code>dims</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dimensions of states and actions.</dd>
<dt><strong><code>cost_function</code></strong> :&ensp;<code>BaseMultiCostFunction</code></dt>
<dd>Multi-cost function.</dd>
<dt><strong><code>nb_costs</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of cost functions</dd>
<dt><strong><code>use_constraints</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the algorithm uses constraints.</dd>
<dt><strong><code>pareto_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of random goals to be sampled to generate the pareto front.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DQN(BaseAlgorithm):
    def __init__(self, env, params):
        &#34;&#34;&#34;
        DQN algorithm from Mnih et al., 2015.
        This implementation includes mechanisms from Universal Value Function Approximators (Schaul et al., 2015)
        and Agent57 (Badia et al., 2019).

        Parameters
        ----------
        env: BaseEnv
            Learning environment.
        params: dict
            Dictionary of parameters.

        Attributes
        ----------
        batch_size: int
            Batch size.
        gamma: float
            Discount factor in [0, 1].
        layers: tuple of ints
            Describes sizes of hidden layers of the critics.
        goal_conditioned: bool
            Whether the algorithm is goal conditioned or not. This is the idea behind UVFA (Schaul et al., 2015).
        replace_target_cnt: int
            Frequency with which target critics should be replaced by current critics (in learning steps).
        logdir: str
            Logging directory
        save_policy_every: int
            Frequency to save policy (in episodes).
        eval_and_log_every: int
            Frequency to pring logs (in episodes).
        n_evals_if_stochastic: int
            Number of evaluation episodes if the environment is stochastic.
        stochastic: bool
            Whether the environment is stochastic.
        epsilon: float
            Probability to sample a random action (epsilon-greedy exploration). This is set to 0 during evaluation.
        dims: dict
            Dimensions of states and actions.
        cost_function: BaseMultiCostFunction
            Multi-cost function.
        nb_costs: int
            Number of cost functions
        use_constraints: bool
            Whether the algorithm uses constraints.
        pareto_size: int
            Number of random goals to be sampled to generate the pareto front.
        &#34;&#34;&#34;
        super(DQN, self).__init__(env, params)

        # Save parameters
        self.batch_size = self.algo_params[&#39;batch_size&#39;]
        self.gamma = self.algo_params[&#39;gamma&#39;]
        self.layers = tuple(self.algo_params[&#39;layers&#39;])
        self.goal_conditioned = self.algo_params[&#39;goal_conditioned&#39;]
        self.replace_target_cnt = self.algo_params[&#39;replace_target_count&#39;]
        self.logdir = params[&#39;logdir&#39;]
        self.save_policy_every = self.algo_params[&#39;save_policy_every&#39;]
        self.eval_and_log_every = self.algo_params[&#39;eval_and_log_every&#39;]
        self.n_evals_if_stochastic = self.algo_params[&#39;n_evals_if_stochastic&#39;]
        self.stochastic = params[&#39;model_params&#39;][&#39;stochastic&#39;]
        self.epsilon = self.algo_params[&#39;epsilon_greedy&#39;]
        self.cost_function = self.env.unwrapped.cost_function
        self.nb_costs = self.env.unwrapped.cost_function.nb_costs
        self.use_constraints = self.cost_function.use_constraints
        self.pareto_size = self.algo_params[&#39;pareto_size&#39;]
        self.is_multi_obj = True if self.goal_conditioned else False  # DQN is not a multi-obj algorithm, unless it is goal-conditioned
        self.dims = dict(s=env.observation_space.shape[0],
                         a=env.action_space.n)

        #

        if self.goal_conditioned:
            self.goal_dim = self.env.unwrapped.cost_function.goal_dim
            eval_goals = self.cost_function.get_eval_goals(1)
            goals, index, inverse = np.unique(eval_goals, return_inverse=True, return_index=True, axis=0)
            goal_keys = [str(g) for g in goals]
        else:
            self.goal_dim = 0
            goal_keys = [str(self.cost_function.beta)]

        # Initialize Logger.
        if self.logdir:
            os.makedirs(self.logdir + &#39;models/&#39;, exist_ok=True)
            stats_keys = [&#39;mean_agg&#39;, &#39;std_agg&#39;] + [&#39;mean_C{}&#39;.format(i) for i in range(self.nb_costs)] + [&#39;std_C{}&#39;.format(i) for i in range(self.nb_costs)]

            keys = [&#39;Episode&#39;, &#39;Best score so far&#39;, &#39;Eval score&#39;]
            for k in goal_keys:
                for s in stats_keys:
                    keys.append(&#39;Eval, g: &#39; + k + &#39;: &#39; + s)
            keys += [&#39;Loss {}&#39;.format(i + 1) for i in range(self.nb_costs)] + [&#39;Train, Cost {}&#39;.format(i + 1) for i in range(self.nb_costs)] + [&#39;Train, Aggregated cost&#39;]
            self.logger = Logger(keys=keys,
                                 logdir=self.logdir)

        # Initialize replay buffer
        self.replay_buffer = ReplayBuffer(self.algo_params[&#39;buffer_size&#39;])

        # Initialize critics
        self.Q_eval = Critic(n_critics=self.nb_costs,
                             dim_state=self.dims[&#39;s&#39;],
                             dim_goal=self.goal_dim,
                             dim_actions=self.dims[&#39;a&#39;],
                             goal_ids=((), ()),  # no goal, the mixing parameters comes after the critic
                             layers=self.layers)
        self.Q_next = Critic(n_critics=self.nb_costs,
                             dim_state=self.dims[&#39;s&#39;],
                             dim_goal=self.goal_dim,
                             dim_actions=self.dims[&#39;a&#39;],
                             goal_ids=((), ()),
                             layers=self.layers)

        # Initialize optimizers.
        self.optimizers = [optim.Adam(q.parameters(), lr=self.algo_params[&#39;lr&#39;]) for q in self.Q_eval.qs]

        # If we use constraint, we train a Q-network per constraint using a negative reward of -1 whenever the constraint is violated.
        # This network learns to estimate the number of times the constraint will be violated in the future.
        # We then use it to guide action selection, selecting the action that does not violate the constraint when the other does,
        # or the action that minimizes constraint violation when both action lead to constraint violations.
        if self.use_constraints:
            self.nb_constraints = len(self.cost_function.constraints_ids)
            self.Q_eval_constraints = Critic(n_critics=self.nb_constraints,
                                             dim_state=self.dims[&#39;s&#39;],
                                             dim_goal=self.goal_dim,
                                             dim_actions=self.dims[&#39;a&#39;],
                                             goal_ids=self.cost_function.constraints_ids,
                                             layers=self.layers)
            self.Q_next_constraints = Critic(n_critics=self.nb_constraints,
                                             dim_state=self.dims[&#39;s&#39;],
                                             dim_goal=self.goal_dim,
                                             dim_actions=self.dims[&#39;a&#39;],
                                             goal_ids=self.cost_function.constraints_ids,
                                             layers=self.layers)
            self.optimizers_constraints = [optim.Adam(q.parameters(), lr=self.algo_params[&#39;lr&#39;]) for q in self.Q_eval_constraints.qs]
        else:
            self.nb_constraints = 0

        # Initialize counters
        self.learn_step_counter = 0
        self.env_step_counter = 0
        self.episode = 0
        self.best_cost = np.inf
        self.aggregated_costs = []
        self.costs = []

    def _replace_target_network(self):
        &#34;&#34;&#34;
        Replaces the target network with the evaluation network every &#39;self.replace_target_cnt&#39; learning steps.
        &#34;&#34;&#34;
        if self.replace_target_cnt is not None and self.learn_step_counter % self.replace_target_cnt == 0:
            self.Q_next.set_goal_params(self.Q_eval.get_params())
        if self.use_constraints:
            if self.replace_target_cnt is not None and self.learn_step_counter % self.replace_target_cnt == 0:
                self.Q_next_constraints.set_goal_params(self.Q_eval_constraints.get_params())

    def _update(self, batch_size):
        &#34;&#34;&#34;
        Performs network updates according to the DQN algorithm.
        Here we update several critics: one for each cost and one for each constraint.
        We then use these critics at decision time: first filtering actions that are not expected to violate constraints,
        then selecting actions that maximize a convex combination of the costs as expressed by the mixing parameter beta.
        Beta can be provided by the experimenter (dqn) or selected by the agent (goal_dqn).

        Parameters
        ----------
        batch_size: int
            Batch size

        Returns
        -------
        loss

        &#34;&#34;&#34;

        # Reset gradients of optimizes
        for opt in self.optimizers:
            opt.zero_grad()

        if self.use_constraints:
            for opt in self.optimizers_constraints:
                opt.zero_grad()

        # Update target network.
        self._replace_target_network()

        # Sample a batch
        state, action, cost_aggregated, costs, next_state, goal, done, constraints = self.replay_buffer.sample(batch_size)

        # Concatenate goal if the policy is goal conditioned (might not be used afterwards).
        if self.goal_conditioned:
            state = ag.Variable(torch.FloatTensor(np.float32(np.concatenate([state, goal], axis=1))))
            next_state = ag.Variable(torch.FloatTensor(np.float32(np.concatenate([next_state, goal], axis=1))))
        else:
            state = ag.Variable(torch.FloatTensor(np.float32(state)))
            next_state = ag.Variable(torch.FloatTensor(np.float32(next_state)))

        action = ag.Variable(torch.LongTensor(action))
        indices = np.arange(self.batch_size)

        rewards = [- ag.Variable(torch.FloatTensor(c_func.scale(c))) for c_func, c in zip(self.cost_function.costs, costs.transpose())]


        q_preds = self.Q_eval.forward(state)
        q_preds = [q_p[indices, action] for q_p in q_preds]
        q_nexts = self.Q_next.forward(next_state)
        q_evals = self.Q_eval.forward(next_state)

        max_actions = [torch.argmax(q_ev, dim=1) for q_ev in q_evals]

        q_targets = [r + self.gamma * q_nex[indices, max_act] for r, q_nex, max_act in zip(rewards, q_nexts, max_actions)]
        losses = [(q_pre - ag.Variable(q_targ.data)).pow(2).mean() for q_pre, q_targ in zip(q_preds, q_targets)]
        for loss in losses:
            loss.backward()

        for opt in self.optimizers:
            opt.step()

        if self.use_constraints:
            constraints = [ag.Variable(torch.FloatTensor(constraints[:, i])) for i in range(self.nb_constraints)]

            q_preds = list(self.Q_eval_constraints.forward(state))
            q_preds = [q_p[indices, action.squeeze()] for q_p in q_preds]
            q_nexts = self.Q_next_constraints.forward(next_state)
            q_evals = self.Q_eval_constraints.forward(next_state)

            for i_q in range(self.nb_constraints):
                max_actions = torch.argmax(q_evals[i_q], dim=1)
                q_target = - constraints[i_q] + 1 * q_nexts[i_q][indices, max_actions]
                losses.append((q_preds[i_q] - ag.Variable(q_target.data)).pow(2).mean())
                losses[-1].backward()

            for opt in self.optimizers_constraints:
                opt.step()
        self.learn_step_counter += 1
        return losses

    def store_episodes(self, episodes):
        lengths = []
        for e in episodes:
            for t in range(e[&#39;env_states&#39;].shape[0] - 1):
                self.replay_buffer.push(state=e[&#39;env_states&#39;][t],
                                        action=e[&#39;actions&#39;][t],
                                        aggregated_cost=e[&#39;aggregated_costs&#39;][t],
                                        costs=e[&#39;costs&#39;][t],
                                        next_state=e[&#39;env_states&#39;][t + 1],
                                        constraints=e[&#39;constraints&#39;][t],
                                        goal=e[&#39;goal&#39;],
                                        done=e[&#39;dones&#39;][t])
            lengths.append(e[&#39;env_states&#39;].shape[0] - 1)
        return lengths

    def act(self, state, deterministic=False):
        &#34;&#34;&#34;
        Policy that uses the learned critics.

        Parameters
        ----------
        state: 1D nd.array
            Current state.
        deterministic: bool
            Whether the policy should be deterministic (e.g. in evaluation mode).

        Returns
        -------
        action: nd.array
            Action vector.
        q_constraints: nd.array
            Values of the critics estimating the expected constraint violations.
        &#34;&#34;&#34;
        if np.random.rand() &gt; self.epsilon or deterministic:
            if self.use_constraints:
                # If we use constraint, then the set of action is filtered by the constraint critics
                # so that we pick an action that is not expected to lead to constraint violation.
                # In the remaining actions, we take the one that maximizes the mixture of critics
                # that evaluate the values of each negative costs expected in the future.
                # If all actions lead to constraint violation, we chose the one that minimizes it.
                with torch.no_grad():
                    state = ag.Variable(torch.FloatTensor(state).unsqueeze(0))
                    q_value1, q_value2 = self.Q_eval.forward(state)
                    beta = self.cost_function.beta
                    q_constraints = torch.cat(self.Q_eval_constraints.forward(state)).numpy()
                    q_constraints_clipped = q_constraints.clip(max=0) # clamp to 0 (q value must be neg)
                    q_constraints_worst = q_constraints_clipped.min(axis=0)
                    valid_ids = np.argwhere(q_constraints_worst &gt; -1).flatten()
                    if valid_ids.size == 0:
                        action = np.argmax(q_constraints.sum(axis=0))
                    else:
                        q_value = (1 - beta) * q_value1[0, valid_ids] + beta * q_value2[0, valid_ids]
                        action = valid_ids[np.argmax(q_value.numpy())]
            else:
                # If no constraint, then the best action is the one that maximizes
                # the mixture of values with the chose mixing parameter beta (either by experimenter or by agent).
                with torch.no_grad():
                    state = ag.Variable(torch.FloatTensor(state).unsqueeze(0))
                    q_value1, q_value2 = self.Q_eval.forward(state)
                    beta = self.cost_function.beta
                    q_value = (1 - beta) * q_value1 + beta * q_value2
                action = int(q_value.max(1)[1].data[0])
                q_constraints = None
        else:
            # Epsilon-greedy exploration, random action with probability epsilon.
            action = np.random.randint(self.dims[&#39;a&#39;])
            q_constraints = None
        return np.atleast_1d(action), q_constraints

    def update(self):
        &#34;&#34;&#34;
        Update the algorithm.

        &#34;&#34;&#34;
        if self.env_step_counter &gt; 0:
            losses = self._update(self.batch_size)
            return [np.atleast_1d(l.data)[0] for l in losses]
        else:
            return [0] * (2 + self.nb_constraints)

    def save_model(self, path):
        &#34;&#34;&#34;
        Extract model state dicts and save them.
        Parameters
        ----------
        path: str
            Saving path.

        &#34;&#34;&#34;
        q_eval = self.Q_eval.get_model()
        to_save = [q_eval]
        if self.use_constraints:
            q_constraints = self.Q_eval_constraints.get_model()
            to_save.append(q_constraints)
        with open(path, &#39;wb&#39;) as f:
            torch.save(to_save, f)

    def load_model(self, path):
        &#34;&#34;&#34;
        Load model from file and feed critics&#39; state dicts.
        Parameters
        ----------
        path: str
            Loading path
        &#34;&#34;&#34;
        with open(path, &#39;rb&#39;) as f:
            out = torch.load(f)
        try:
            self.Q_eval.set_model(out[0])
        except:
            self.Q_eval.set_model(out)

        if self.use_constraints:
            self.Q_eval_constraints.set_model(out[1])

    def learn(self, num_train_steps):
        &#34;&#34;&#34;
        Main training loop.

        Parameters
        ----------
        num_train_steps: int
            Number of training steps (environment steps)

        Returns
        -------

        &#34;&#34;&#34;

        while self.env_step_counter &lt; num_train_steps:
            if self.goal_conditioned:
                goal = self.env.unwrapped.sample_cost_function_params()
            else:
                goal = None

            episodes = run_rollout(policy=self,
                                   env=self.env,
                                   n=1,
                                   goal=goal,
                                   eval=False,
                                   additional_keys=(&#39;costs&#39;, &#39;constraints&#39;),
                                   )
            lengths = self.store_episodes(episodes)
            self.env_step_counter += np.sum(lengths)
            self.episode += 1

            self.aggregated_costs.append(np.sum(episodes[0][&#39;aggregated_costs&#39;]))
            self.costs.append(np.sum(episodes[0][&#39;costs&#39;], axis=0))

            # Update
            if len(self.replay_buffer) &gt; self.batch_size:
                update_losses = []
                for _ in range(int(np.sum(lengths) * 0.5)):
                    update_losses.append(self.update())
                update_losses = np.array(update_losses)
                losses = update_losses.mean(axis=0)
            else:
                losses = [np.nan] * 2

            if self.episode % self.eval_and_log_every == 0:
                # Run evaluations
                new_logs, eval_costs = self.evaluate(n=self.n_evals_if_stochastic if self.stochastic else 1)
                # Compute train scores
                train_agg_cost = np.mean(self.aggregated_costs)
                train_costs = np.array(self.costs).mean(axis=0)
                self.log(self.episode, new_logs, losses, train_agg_cost, train_costs)
                # Reset training score tracking
                self.aggregated_costs = []
                self.costs = []

            if self.episode % self.save_policy_every == 0:
                self.save_model(self.logdir + &#39;/models/policy_{}.cp&#39;.format(self.episode))
        self.evaluate_pareto()
        print(&#39;Run has terminated successfully&#39;)

    def evaluate(self, n=None, goal=None, best=None, reset_same_model=False):
        # run eval
        if n is None:
            n = self.n_evals_if_stochastic if self.env.unwrapped.stochastic else 1
        if self.goal_conditioned:
            if goal is not None:
                eval_goals = np.array([goal] * n)
            else:
                eval_goals = self.cost_function.get_eval_goals(n)

            n = eval_goals.shape[0]
        else:
            eval_goals = None
        eval_episodes = run_rollout(policy=self,
                                    env=self.env,
                                    n=n,
                                    goal=eval_goals,
                                    eval=True,
                                    reset_same_model=reset_same_model,
                                    additional_keys=(&#39;costs&#39;, &#39;constraints&#39;),
                                    )
        new_logs, costs = self.compute_eval_score(eval_episodes, eval_goals)
        return new_logs, costs

    def compute_eval_score(self, eval_episodes, eval_goals):
        aggregated_costs = [np.sum(e[&#39;aggregated_costs&#39;]) for e in eval_episodes]
        costs = np.array([np.sum(e[&#39;costs&#39;], axis=0) for e in eval_episodes])

        new_logs = dict()
        if self.goal_conditioned:
            goals, index, inverse = np.unique(eval_goals, return_inverse=True, return_index=True, axis=0)
            agg_means = []
            for g, i in zip(goals, np.arange(index.size)):
                ind_g = np.argwhere(inverse == i).flatten()
                costs_mean = np.mean(costs[ind_g], axis=0)
                costs_std = np.std(costs[ind_g], axis=0)
                agg_rew_mean = np.mean(np.array(aggregated_costs)[ind_g], axis=0)
                agg_rew_std = np.std(np.array(aggregated_costs)[ind_g], axis=0)
                for i_r in range(self.nb_costs):
                    new_logs[&#39;Eval, g: &#39; + str(g) + &#39;: &#39; + &#39;mean_C{}&#39;.format(i_r)] = costs_mean[i_r]
                    new_logs[&#39;Eval, g: &#39; + str(g) + &#39;: &#39; + &#39;std_C{}&#39;.format(i_r)] = costs_std[i_r]
                new_logs[&#39;Eval, g: &#39; + str(g) + &#39;: &#39; + &#39;mean_agg&#39;] = agg_rew_mean
                new_logs[&#39;Eval, g: &#39; + str(g) + &#39;: &#39; + &#39;std_agg&#39;] = agg_rew_std
                agg_means.append(agg_rew_mean)
            new_logs[&#39;Eval score&#39;] = np.mean(agg_means)
        else:
            costs_mean = np.mean(np.atleast_2d(costs), axis=0)
            costs_std = np.std(np.atleast_2d(costs), axis=0)
            for i_r in range(self.nb_costs):
                new_logs[&#39;Eval, g: &#39; + str(self.cost_function.beta) + &#39;: &#39; + &#39;mean_C{}&#39;.format(i_r)] = costs_mean[i_r]
                new_logs[&#39;Eval, g: &#39; + str(self.cost_function.beta) + &#39;: &#39; + &#39;std_C{}&#39;.format(i_r)] = costs_std[i_r]
            new_logs[&#39;Eval score&#39;] = np.mean(aggregated_costs)
            new_logs[&#39;Eval, g: &#39; + str(self.cost_function.beta) + &#39;: &#39; + &#39;mean_agg&#39;] = np.mean(aggregated_costs)
            new_logs[&#39;Eval, g: &#39; + str(self.cost_function.beta) + &#39;: &#39; + &#39;std_agg&#39;] = np.mean(aggregated_costs)

        return new_logs, costs

    def log(self, episode, new_logs, losses, train_agg_cost, train_costs):
        if new_logs[&#39;Eval score&#39;] &lt; self.best_cost:
            self.best_cost = new_logs[&#39;Eval score&#39;]
            self.save_model(self.logdir + &#39;/models/best_model.cp&#39;)

        train_log_dict = {&#39;Episode&#39;: episode, &#39;Best score so far&#39;: self.best_cost}
        for i in range(self.nb_costs):
            train_log_dict[&#39;Loss {}&#39;.format(i + 1)] = losses[i]
            train_log_dict[&#39;Train, Cost {}&#39;.format(i + 1)] = train_costs[i]
            train_log_dict[&#39;Train, Aggregated cost&#39;] = train_agg_cost

        new_logs.update(train_log_dict)
        self.logger.add(new_logs)
        self.logger.print_last()
        self.logger.save()

    def evaluate_pareto(self, load_model=True):
        if load_model:
            self.load_model(self.logdir + &#39;/models/best_model.cp&#39;)
        if self.goal_conditioned:
            print(&#39;----------------\nForming pareto front&#39;)
            goals = sample_goals(self.pareto_size, self.cost_function.goal_dim)

            res = dict()

            costs_mean = []
            costs_std = []
            n = self.n_evals_if_stochastic if self.env.unwrapped.stochastic else 1
            for i_g, g in enumerate(goals):
                if (i_g + 1) % 20 == 0:
                    print(&#39;\t{:.2f} %&#39;.format((i_g + 1)/goals.shape[0] * 100))
                gs = np.atleast_2d(np.array([g for _ in range(n)]))
                if gs.shape[0] != n:
                    gs = gs.transpose()
                episodes = run_rollout(policy=self,
                                       env=self.env,
                                       n=n,
                                       goal=gs,
                                       eval=True,
                                       additional_keys=[&#39;costs&#39;],
                                       )

                costs = np.array([np.array(e[&#39;costs&#39;]).sum(axis=0) for e in episodes])
                costs_mean.append(costs.mean(axis=0))
                costs_std.append(costs.std(axis=0))
            res[&#39;F_all&#39;] = np.array(costs_mean)
            res[&#39;F_std_all&#39;] = np.array(costs_std)
            res[&#39;G_all&#39;] = goals

            front_ids = compute_pareto_front(costs_mean)
            costs_mean = np.array(costs_mean)
            costs_std = np.array(costs_std)
            costs_std = costs_std[front_ids]
            costs_mean = costs_mean[front_ids]
            res[&#39;F&#39;] = costs_mean
            res[&#39;F_std&#39;] = costs_std

            with open(self.logdir + &#39;res_eval.pk&#39;, &#39;wb&#39;) as f:
                pickle.dump(res, f)
        else:
            print(&#39;----------------\nForming pareto front&#39;)

            res = dict()
            costs_mean = []
            costs_std = []
            n = self.n_evals_if_stochastic if self.env.unwrapped.stochastic else 1
            episodes = run_rollout(policy=self,
                                   env=self.env,
                                   n=n,
                                   eval=True,
                                   additional_keys=[&#39;costs&#39;],
                                   )

            costs = np.array([np.array(e[&#39;costs&#39;]).sum(axis=0) for e in episodes])
            costs_mean.append(costs.mean(axis=0))
            costs_std.append(costs.std(axis=0))
            res[&#39;F&#39;] = np.array(costs_mean)
            res[&#39;F_std&#39;] = np.array(costs_std)
            for k in list(res.keys()):
                res[k + &#39;_all&#39;] = res[k]
            res[&#39;G_all&#39;] = np.array([[self.cost_function.beta_default for _ in range(len(costs_mean))]])

            with open(self.logdir + &#39;res_eval.pk&#39;, &#39;wb&#39;) as f:
                pickle.dump(res, f)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="epidemioptim.optimization.base_algorithm.BaseAlgorithm" href="../base_algorithm.html#epidemioptim.optimization.base_algorithm.BaseAlgorithm">BaseAlgorithm</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="epidemioptim.optimization.dqn.dqn.DQN.act"><code class="name flex">
<span>def <span class="ident">act</span></span>(<span>self, state, deterministic=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Policy that uses the learned critics.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>1D nd.array</code></dt>
<dd>Current state.</dd>
<dt><strong><code>deterministic</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the policy should be deterministic (e.g. in evaluation mode).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>action</code></strong> :&ensp;<code>nd.array</code></dt>
<dd>Action vector.</dd>
<dt><strong><code>q_constraints</code></strong> :&ensp;<code>nd.array</code></dt>
<dd>Values of the critics estimating the expected constraint violations.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def act(self, state, deterministic=False):
    &#34;&#34;&#34;
    Policy that uses the learned critics.

    Parameters
    ----------
    state: 1D nd.array
        Current state.
    deterministic: bool
        Whether the policy should be deterministic (e.g. in evaluation mode).

    Returns
    -------
    action: nd.array
        Action vector.
    q_constraints: nd.array
        Values of the critics estimating the expected constraint violations.
    &#34;&#34;&#34;
    if np.random.rand() &gt; self.epsilon or deterministic:
        if self.use_constraints:
            # If we use constraint, then the set of action is filtered by the constraint critics
            # so that we pick an action that is not expected to lead to constraint violation.
            # In the remaining actions, we take the one that maximizes the mixture of critics
            # that evaluate the values of each negative costs expected in the future.
            # If all actions lead to constraint violation, we chose the one that minimizes it.
            with torch.no_grad():
                state = ag.Variable(torch.FloatTensor(state).unsqueeze(0))
                q_value1, q_value2 = self.Q_eval.forward(state)
                beta = self.cost_function.beta
                q_constraints = torch.cat(self.Q_eval_constraints.forward(state)).numpy()
                q_constraints_clipped = q_constraints.clip(max=0) # clamp to 0 (q value must be neg)
                q_constraints_worst = q_constraints_clipped.min(axis=0)
                valid_ids = np.argwhere(q_constraints_worst &gt; -1).flatten()
                if valid_ids.size == 0:
                    action = np.argmax(q_constraints.sum(axis=0))
                else:
                    q_value = (1 - beta) * q_value1[0, valid_ids] + beta * q_value2[0, valid_ids]
                    action = valid_ids[np.argmax(q_value.numpy())]
        else:
            # If no constraint, then the best action is the one that maximizes
            # the mixture of values with the chose mixing parameter beta (either by experimenter or by agent).
            with torch.no_grad():
                state = ag.Variable(torch.FloatTensor(state).unsqueeze(0))
                q_value1, q_value2 = self.Q_eval.forward(state)
                beta = self.cost_function.beta
                q_value = (1 - beta) * q_value1 + beta * q_value2
            action = int(q_value.max(1)[1].data[0])
            q_constraints = None
    else:
        # Epsilon-greedy exploration, random action with probability epsilon.
        action = np.random.randint(self.dims[&#39;a&#39;])
        q_constraints = None
    return np.atleast_1d(action), q_constraints</code></pre>
</details>
</dd>
<dt id="epidemioptim.optimization.dqn.dqn.DQN.compute_eval_score"><code class="name flex">
<span>def <span class="ident">compute_eval_score</span></span>(<span>self, eval_episodes, eval_goals)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_eval_score(self, eval_episodes, eval_goals):
    aggregated_costs = [np.sum(e[&#39;aggregated_costs&#39;]) for e in eval_episodes]
    costs = np.array([np.sum(e[&#39;costs&#39;], axis=0) for e in eval_episodes])

    new_logs = dict()
    if self.goal_conditioned:
        goals, index, inverse = np.unique(eval_goals, return_inverse=True, return_index=True, axis=0)
        agg_means = []
        for g, i in zip(goals, np.arange(index.size)):
            ind_g = np.argwhere(inverse == i).flatten()
            costs_mean = np.mean(costs[ind_g], axis=0)
            costs_std = np.std(costs[ind_g], axis=0)
            agg_rew_mean = np.mean(np.array(aggregated_costs)[ind_g], axis=0)
            agg_rew_std = np.std(np.array(aggregated_costs)[ind_g], axis=0)
            for i_r in range(self.nb_costs):
                new_logs[&#39;Eval, g: &#39; + str(g) + &#39;: &#39; + &#39;mean_C{}&#39;.format(i_r)] = costs_mean[i_r]
                new_logs[&#39;Eval, g: &#39; + str(g) + &#39;: &#39; + &#39;std_C{}&#39;.format(i_r)] = costs_std[i_r]
            new_logs[&#39;Eval, g: &#39; + str(g) + &#39;: &#39; + &#39;mean_agg&#39;] = agg_rew_mean
            new_logs[&#39;Eval, g: &#39; + str(g) + &#39;: &#39; + &#39;std_agg&#39;] = agg_rew_std
            agg_means.append(agg_rew_mean)
        new_logs[&#39;Eval score&#39;] = np.mean(agg_means)
    else:
        costs_mean = np.mean(np.atleast_2d(costs), axis=0)
        costs_std = np.std(np.atleast_2d(costs), axis=0)
        for i_r in range(self.nb_costs):
            new_logs[&#39;Eval, g: &#39; + str(self.cost_function.beta) + &#39;: &#39; + &#39;mean_C{}&#39;.format(i_r)] = costs_mean[i_r]
            new_logs[&#39;Eval, g: &#39; + str(self.cost_function.beta) + &#39;: &#39; + &#39;std_C{}&#39;.format(i_r)] = costs_std[i_r]
        new_logs[&#39;Eval score&#39;] = np.mean(aggregated_costs)
        new_logs[&#39;Eval, g: &#39; + str(self.cost_function.beta) + &#39;: &#39; + &#39;mean_agg&#39;] = np.mean(aggregated_costs)
        new_logs[&#39;Eval, g: &#39; + str(self.cost_function.beta) + &#39;: &#39; + &#39;std_agg&#39;] = np.mean(aggregated_costs)

    return new_logs, costs</code></pre>
</details>
</dd>
<dt id="epidemioptim.optimization.dqn.dqn.DQN.evaluate_pareto"><code class="name flex">
<span>def <span class="ident">evaluate_pareto</span></span>(<span>self, load_model=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_pareto(self, load_model=True):
    if load_model:
        self.load_model(self.logdir + &#39;/models/best_model.cp&#39;)
    if self.goal_conditioned:
        print(&#39;----------------\nForming pareto front&#39;)
        goals = sample_goals(self.pareto_size, self.cost_function.goal_dim)

        res = dict()

        costs_mean = []
        costs_std = []
        n = self.n_evals_if_stochastic if self.env.unwrapped.stochastic else 1
        for i_g, g in enumerate(goals):
            if (i_g + 1) % 20 == 0:
                print(&#39;\t{:.2f} %&#39;.format((i_g + 1)/goals.shape[0] * 100))
            gs = np.atleast_2d(np.array([g for _ in range(n)]))
            if gs.shape[0] != n:
                gs = gs.transpose()
            episodes = run_rollout(policy=self,
                                   env=self.env,
                                   n=n,
                                   goal=gs,
                                   eval=True,
                                   additional_keys=[&#39;costs&#39;],
                                   )

            costs = np.array([np.array(e[&#39;costs&#39;]).sum(axis=0) for e in episodes])
            costs_mean.append(costs.mean(axis=0))
            costs_std.append(costs.std(axis=0))
        res[&#39;F_all&#39;] = np.array(costs_mean)
        res[&#39;F_std_all&#39;] = np.array(costs_std)
        res[&#39;G_all&#39;] = goals

        front_ids = compute_pareto_front(costs_mean)
        costs_mean = np.array(costs_mean)
        costs_std = np.array(costs_std)
        costs_std = costs_std[front_ids]
        costs_mean = costs_mean[front_ids]
        res[&#39;F&#39;] = costs_mean
        res[&#39;F_std&#39;] = costs_std

        with open(self.logdir + &#39;res_eval.pk&#39;, &#39;wb&#39;) as f:
            pickle.dump(res, f)
    else:
        print(&#39;----------------\nForming pareto front&#39;)

        res = dict()
        costs_mean = []
        costs_std = []
        n = self.n_evals_if_stochastic if self.env.unwrapped.stochastic else 1
        episodes = run_rollout(policy=self,
                               env=self.env,
                               n=n,
                               eval=True,
                               additional_keys=[&#39;costs&#39;],
                               )

        costs = np.array([np.array(e[&#39;costs&#39;]).sum(axis=0) for e in episodes])
        costs_mean.append(costs.mean(axis=0))
        costs_std.append(costs.std(axis=0))
        res[&#39;F&#39;] = np.array(costs_mean)
        res[&#39;F_std&#39;] = np.array(costs_std)
        for k in list(res.keys()):
            res[k + &#39;_all&#39;] = res[k]
        res[&#39;G_all&#39;] = np.array([[self.cost_function.beta_default for _ in range(len(costs_mean))]])

        with open(self.logdir + &#39;res_eval.pk&#39;, &#39;wb&#39;) as f:
            pickle.dump(res, f)</code></pre>
</details>
</dd>
<dt id="epidemioptim.optimization.dqn.dqn.DQN.learn"><code class="name flex">
<span>def <span class="ident">learn</span></span>(<span>self, num_train_steps)</span>
</code></dt>
<dd>
<div class="desc"><p>Main training loop.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>num_train_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of training steps (environment steps)</dd>
</dl>
<h2 id="returns">Returns</h2></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn(self, num_train_steps):
    &#34;&#34;&#34;
    Main training loop.

    Parameters
    ----------
    num_train_steps: int
        Number of training steps (environment steps)

    Returns
    -------

    &#34;&#34;&#34;

    while self.env_step_counter &lt; num_train_steps:
        if self.goal_conditioned:
            goal = self.env.unwrapped.sample_cost_function_params()
        else:
            goal = None

        episodes = run_rollout(policy=self,
                               env=self.env,
                               n=1,
                               goal=goal,
                               eval=False,
                               additional_keys=(&#39;costs&#39;, &#39;constraints&#39;),
                               )
        lengths = self.store_episodes(episodes)
        self.env_step_counter += np.sum(lengths)
        self.episode += 1

        self.aggregated_costs.append(np.sum(episodes[0][&#39;aggregated_costs&#39;]))
        self.costs.append(np.sum(episodes[0][&#39;costs&#39;], axis=0))

        # Update
        if len(self.replay_buffer) &gt; self.batch_size:
            update_losses = []
            for _ in range(int(np.sum(lengths) * 0.5)):
                update_losses.append(self.update())
            update_losses = np.array(update_losses)
            losses = update_losses.mean(axis=0)
        else:
            losses = [np.nan] * 2

        if self.episode % self.eval_and_log_every == 0:
            # Run evaluations
            new_logs, eval_costs = self.evaluate(n=self.n_evals_if_stochastic if self.stochastic else 1)
            # Compute train scores
            train_agg_cost = np.mean(self.aggregated_costs)
            train_costs = np.array(self.costs).mean(axis=0)
            self.log(self.episode, new_logs, losses, train_agg_cost, train_costs)
            # Reset training score tracking
            self.aggregated_costs = []
            self.costs = []

        if self.episode % self.save_policy_every == 0:
            self.save_model(self.logdir + &#39;/models/policy_{}.cp&#39;.format(self.episode))
    self.evaluate_pareto()
    print(&#39;Run has terminated successfully&#39;)</code></pre>
</details>
</dd>
<dt id="epidemioptim.optimization.dqn.dqn.DQN.load_model"><code class="name flex">
<span>def <span class="ident">load_model</span></span>(<span>self, path)</span>
</code></dt>
<dd>
<div class="desc"><p>Load model from file and feed critics' state dicts.
Parameters</p>
<hr>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Loading path</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model(self, path):
    &#34;&#34;&#34;
    Load model from file and feed critics&#39; state dicts.
    Parameters
    ----------
    path: str
        Loading path
    &#34;&#34;&#34;
    with open(path, &#39;rb&#39;) as f:
        out = torch.load(f)
    try:
        self.Q_eval.set_model(out[0])
    except:
        self.Q_eval.set_model(out)

    if self.use_constraints:
        self.Q_eval_constraints.set_model(out[1])</code></pre>
</details>
</dd>
<dt id="epidemioptim.optimization.dqn.dqn.DQN.save_model"><code class="name flex">
<span>def <span class="ident">save_model</span></span>(<span>self, path)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract model state dicts and save them.
Parameters</p>
<hr>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Saving path.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_model(self, path):
    &#34;&#34;&#34;
    Extract model state dicts and save them.
    Parameters
    ----------
    path: str
        Saving path.

    &#34;&#34;&#34;
    q_eval = self.Q_eval.get_model()
    to_save = [q_eval]
    if self.use_constraints:
        q_constraints = self.Q_eval_constraints.get_model()
        to_save.append(q_constraints)
    with open(path, &#39;wb&#39;) as f:
        torch.save(to_save, f)</code></pre>
</details>
</dd>
<dt id="epidemioptim.optimization.dqn.dqn.DQN.store_episodes"><code class="name flex">
<span>def <span class="ident">store_episodes</span></span>(<span>self, episodes)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def store_episodes(self, episodes):
    lengths = []
    for e in episodes:
        for t in range(e[&#39;env_states&#39;].shape[0] - 1):
            self.replay_buffer.push(state=e[&#39;env_states&#39;][t],
                                    action=e[&#39;actions&#39;][t],
                                    aggregated_cost=e[&#39;aggregated_costs&#39;][t],
                                    costs=e[&#39;costs&#39;][t],
                                    next_state=e[&#39;env_states&#39;][t + 1],
                                    constraints=e[&#39;constraints&#39;][t],
                                    goal=e[&#39;goal&#39;],
                                    done=e[&#39;dones&#39;][t])
        lengths.append(e[&#39;env_states&#39;].shape[0] - 1)
    return lengths</code></pre>
</details>
</dd>
<dt id="epidemioptim.optimization.dqn.dqn.DQN.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the algorithm.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#34;&#34;&#34;
    Update the algorithm.

    &#34;&#34;&#34;
    if self.env_step_counter &gt; 0:
        losses = self._update(self.batch_size)
        return [np.atleast_1d(l.data)[0] for l in losses]
    else:
        return [0] * (2 + self.nb_constraints)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="epidemioptim.optimization.base_algorithm.BaseAlgorithm" href="../base_algorithm.html#epidemioptim.optimization.base_algorithm.BaseAlgorithm">BaseAlgorithm</a></b></code>:
<ul class="hlist">
<li><code><a title="epidemioptim.optimization.base_algorithm.BaseAlgorithm.evaluate" href="../base_algorithm.html#epidemioptim.optimization.base_algorithm.BaseAlgorithm.evaluate">evaluate</a></code></li>
<li><code><a title="epidemioptim.optimization.base_algorithm.BaseAlgorithm.log" href="../base_algorithm.html#epidemioptim.optimization.base_algorithm.BaseAlgorithm.log">log</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="epidemioptim.optimization.dqn" href="index.html">epidemioptim.optimization.dqn</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="epidemioptim.optimization.dqn.dqn.sample_goals" href="#epidemioptim.optimization.dqn.dqn.sample_goals">sample_goals</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="epidemioptim.optimization.dqn.dqn.DQN" href="#epidemioptim.optimization.dqn.dqn.DQN">DQN</a></code></h4>
<ul class="two-column">
<li><code><a title="epidemioptim.optimization.dqn.dqn.DQN.act" href="#epidemioptim.optimization.dqn.dqn.DQN.act">act</a></code></li>
<li><code><a title="epidemioptim.optimization.dqn.dqn.DQN.compute_eval_score" href="#epidemioptim.optimization.dqn.dqn.DQN.compute_eval_score">compute_eval_score</a></code></li>
<li><code><a title="epidemioptim.optimization.dqn.dqn.DQN.evaluate_pareto" href="#epidemioptim.optimization.dqn.dqn.DQN.evaluate_pareto">evaluate_pareto</a></code></li>
<li><code><a title="epidemioptim.optimization.dqn.dqn.DQN.learn" href="#epidemioptim.optimization.dqn.dqn.DQN.learn">learn</a></code></li>
<li><code><a title="epidemioptim.optimization.dqn.dqn.DQN.load_model" href="#epidemioptim.optimization.dqn.dqn.DQN.load_model">load_model</a></code></li>
<li><code><a title="epidemioptim.optimization.dqn.dqn.DQN.save_model" href="#epidemioptim.optimization.dqn.dqn.DQN.save_model">save_model</a></code></li>
<li><code><a title="epidemioptim.optimization.dqn.dqn.DQN.store_episodes" href="#epidemioptim.optimization.dqn.dqn.DQN.store_episodes">store_episodes</a></code></li>
<li><code><a title="epidemioptim.optimization.dqn.dqn.DQN.update" href="#epidemioptim.optimization.dqn.dqn.DQN.update">update</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>